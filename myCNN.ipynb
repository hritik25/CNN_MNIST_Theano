{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting line by line Michael Nelson code on his tutorial and adding explainatory comments. Some lines of code needed to be modified to incorporate changes in Theano modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 645M (CNMeM is disabled, cuDNN not available)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
      "to set the GPU flag to False.\n"
     ]
    }
   ],
   "source": [
    "# ALL BOLD COMMENTS ARE 'TO-DOs' ; COME BACK TO COMPLETE THEM\n",
    "# Note: This code is an updated version of what is provided on Michael Nelson's website,\n",
    "# as some of the modules have been moved.\n",
    "\n",
    "# Standard library\n",
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv\n",
    "from theano.tensor.nnet import softmax\n",
    "from theano.tensor import shared_randomstreams\n",
    "from theano.tensor.signal import pool\n",
    "\n",
    "# Activation functions for neurons\n",
    "def linear(z): return z\n",
    "def ReLU(z): return T.maximum(0.0, z)\n",
    "from theano.tensor.nnet import sigmoid\n",
    "from theano.tensor import tanh\n",
    "\n",
    "#### Constants\n",
    "GPU = True\n",
    "if GPU:\n",
    "    print \"Trying to run under a GPU.  If this is not desired, then modify \"+\\\n",
    "        \"network3.py\\nto set the GPU flag to False.\"\n",
    "    try: theano.config.device = 'gpu'\n",
    "    except: pass # it's already set\n",
    "    theano.config.floatX = 'float32'\n",
    "else:\n",
    "    print \"Running with a CPU.  If this is not desired, then the modify \"+\\\n",
    "        \"network3.py to set\\nthe GPU flag to True.\"\n",
    "\n",
    "#### Load the MNIST data\n",
    "def load_data_shared(filename=\"../data/mnist.pkl.gz\"):\n",
    "    f = gzip.open(filename, 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f)\n",
    "    f.close()\n",
    "    def shared(data):\n",
    "\n",
    "        shared_x = theano.shared(\n",
    "            np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
    "        shared_y = theano.shared(\n",
    "            np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
    "        return shared_x, T.cast(shared_y, \"int32\")\n",
    "    return [shared(training_data), shared(validation_data), shared(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class fullyConnectedLayer(object):\n",
    "\n",
    "    def __init__(self, nIn, nOut, activationFn, pDropout=0.0):\n",
    "        self.nIn = nIn\n",
    "        self.nOut = nOut\n",
    "        self.activationFn = activationFn\n",
    "        self.pDropout = pDropout\n",
    "        # Initialize weights and biases\n",
    "        # this weight and bias initialization are designed for the sigmoid activation function\n",
    "        # (as discussed earlier). Ideally, we'd initialize the weights and biases somewhat differently\n",
    "        # for activation functions such as the tanh and rectified linear function\n",
    "        self.w = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(\n",
    "                    loc=0.0, scale=np.sqrt(1.0/nOut), size=(nIn, nOut)),\n",
    "                dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(nOut,)),\n",
    "                       dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    # this function will be used to compute the input of the FullyConnectedLayer\n",
    "    # and to compute the corresponding output\n",
    "    \n",
    "    def setInput(self, inpt, inptDropout, miniBatchSize):\n",
    "        # COME BACK TO COMPLETE IT LATER\n",
    "        self.inpt = inpt.reshape((miniBatchSize, self.nIn))\n",
    "        \n",
    "        self.yOut = T.argax( self.output ,axis = 1)\n",
    "\n",
    "# this is not needed except in the last layer\n",
    "#     def accuracy(self, y):\n",
    "#         \"Return the accuracy for the mini-batch.\"\n",
    "#         return T.mean(T.eq(y, self.yOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class netowrk(object):\n",
    "    \n",
    "    def __init__(self, layers, miniBatchSize):\n",
    "        \"\"\"\n",
    "        It takes a list of layers which describe the network architecture\n",
    "        as the 'layers' argument and a value for 'miniBatchSize' to be used while \n",
    "        training using stochastic gradient descent \"\"\"\n",
    "        self.layers = layers\n",
    "        self.miniBatchSize = miniBatchSize\n",
    "        \n",
    "        # a nested loop for bundling up all the parameters from different layers\n",
    "        self.params = [params for j in self.layers for params in j.params]\n",
    "        \n",
    "        # symbolic matrix variable 'X' will contain the training examples in a \n",
    "        # mini batch, where the columns are the features and rows are training examples\n",
    "        # symbolic vector variable 'y' will contain the corresponding target values\n",
    "        self.X = T.matrix('X')\n",
    "        self.y = T.ivector('y')\n",
    "        inputLayer = self.layers[0]\n",
    "        inputLayer.setInput(self.X, self.X, self.miniBatchSize)\n",
    "        \n",
    "        # propagate through the network architecture\n",
    "        for j in range(1, len(layers)):\n",
    "            previousLayer, currentLayer = layers[j-1], layers[j]\n",
    "            currentLayer.setInput(previousLayer.output, previousLayer.outputDropout, self.miniBatchSize)\n",
    "        \n",
    "        # there will be an output variable which will be the final output of our CNN\n",
    "        self.output = self.layers[-1].output\n",
    "        self.outputDropout = self.layers[-1].outputDropout\n",
    "        \n",
    "    def sgd(self, epochs, trainingData, validationData, testData, eta, regularizationParameter = 0.0):\n",
    "        \"\"\"Train the data using mini-batch stochastic gradient descent\"\"\"\n",
    "        \n",
    "        # get features and target values separately\n",
    "        trainX, trainY = trainingData\n",
    "        testX, testY = testData\n",
    "        validationX, validationY = validationData\n",
    "        \n",
    "        # compute the number of miniBatches for training, validation and testing\n",
    "        \n",
    "        # CURRENTLY I DON'T KNOW THE 'type' OF 'traininData', 'testData', AND 'validationData'\n",
    "        # SO I'M USING 'len' TO GET ITS SIZE\n",
    "        numTrainingBatches = len(trainingData)/miniBatchSize\n",
    "        numValidationBatches = len(validationData)/miniBatchSize\n",
    "        numTestBatches = len(testData)/miniBatchSize\n",
    "        \n",
    "        # symbolic computations\n",
    "        # 1. regularization term in cost ; NOTE: .sum() is for numpy arrays, sum(___) is for python iterables\n",
    "        l2NormalizationSquared = sum([(layer.w**2).sum() for layer in self.layers])\n",
    "        cost = self.layers[-1].cost(self) + 0.5*lmbda*l2NormalizationSquared/numTrainingBatches\n",
    "        grads = T.grad(cost, self.params)\n",
    "        updates = [(param, param-eta*grad) for param in zip(self.params, grads)]\n",
    "        \n",
    "        # functions to train mini-batches and to calculate accuracy in validation\n",
    "        # and test mini-batches\n",
    "        i = T.lscalar() # mini batch number\n",
    "        trainMb = theano.function([i], cost, updates = updates, givens = {\n",
    "                self.X : trainX[i*miniBatchSize : (i+1)*miniBatchSize],\n",
    "                self.y : testY[i*miniBatchSize : (i+1)*miniBatchSize]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        validateMb = theano.function([i], self.layers[-1].accuracy(self.y), givens = {\n",
    "                self.X : validationX[i*miniBatchSize : (i+1)*miniBatchSize],\n",
    "                self.y : validationY[i*miniBatchSize : (i+1)*miniBatchSize]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        testMb = theano.function([i], self.layers[-1].accuracy(self.y), givens = {\n",
    "                self.X : trainX[i*miniBatchSize : (i+1)*miniBatchSize],\n",
    "                self.y : testY[i*miniBatchSize : (i+1)*miniBatchSize]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        bestValidationAccuracy = 0.0\n",
    "        for epoch in xrange(epochs):\n",
    "            for miniBatch in xrange(numTrainingBatches):\n",
    "                iteration = numTrainingBatches*epoch + miniBatch\n",
    "                if iteration%100 == 0:\n",
    "                    print \"This is iteration number {0}\".format(iteration)\n",
    "                cost = trainMb([miniBatch])\n",
    "                if (iteration+1)%numTrainingBatches == 0:\n",
    "                    validationAccuracy = np.mean([validateMb([j]) for j in numValidationBatches])\n",
    "                    if validationAccuracy > bestValidationAccuracy:\n",
    "                        bestIteration = iteration\n",
    "                        print \"This is the best validation accuracy so far : {0}, obtained in iteration number = {1}\".format(validationAccuracy, iteration)\n",
    "                        testAccuracy = np.mean([testMb(j) for j in numTestBatches])\n",
    "                        print \"The corresponding test accuracy is {0}\".format(testAccuracy)\n",
    "                        \n",
    "        print \"Finished training network\"\n",
    "        print \"The best validation accuracy was obtained {0}, was obtained in iteration number {1}\".format(validationAccuracy, iteration)\n",
    "        print \"The corresponding test accuracy was {0}\".format(testAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class convPoolLayer(object):\n",
    "    \"\"\"\n",
    "    will implement a convolutional + pooling layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, filterShape, imageShape, poolSize = (2,2), activationFn = sigmoid):\n",
    "        \n",
    "        # DON'T WORRY ABOUT WHY THESE TWO PARAMETERS ARE THE WAY THEY ARE. \n",
    "        # YOU DID NOT HAVE TO COME UP WITH THIS. IT'S OKAY. IT'S BEING GOVERNED BY\n",
    "        # THEANO IMPLEMENTATION DETAILS\n",
    "        \n",
    "        # `filterShape` is a tuple of length 4, whose entries are the number\n",
    "        # of filters, the number of input feature maps, the filter height, and the\n",
    "        # filter width.\n",
    "        self.filterShape = filterShape\n",
    "        \n",
    "        # 'image_shape` is a tuple of length 4, whose entries are the\n",
    "        # mini-batch size, the number of input feature maps, the image\n",
    "        # height, and the image width.\n",
    "        self.imageShape = imageShape\n",
    "        \n",
    "        self.poolSize = poolSize\n",
    "        self.activationFn = activationFn\n",
    "        \n",
    "        # initialize weights and biases\n",
    "        self.w = theano.shared(np.asarray(np.random.normal(loc = 0, scale = np.sqrt(1.0/nOut), \n",
    "                                                size = filterShape), dtype = theano.config.floatX), borrow = True)\n",
    "        self.b = theano.shared(np.asarray(np.random.normal(loc = 0, scale = 1.0, \n",
    "                                                size = (filterShape[0],)), dtype = theano.config.floatX), borrow = True)\n",
    "        self.params = [self.w, self.b]\n",
    "        \n",
    "    def setInput(self, inpt, inptDropout, miniBatchSize):\n",
    "        self.inpt = inpt.reshape(imageShape)\n",
    "        \n",
    "        # this output will be a 4-tuple ( batch size, # output feature maps, width , height )\n",
    "        convOut = conv.conv2d(input = self. inpt, filters = self.w, filter_shape = self.filterShape, \n",
    "                              input_shape = self.imageShape)\n",
    "        \n",
    "        # 'pool.pool_2d' : Takes as input a N-D tensor, where N >= 2. It downscales the input image by the \n",
    "        # specified factor, by keeping only the maximum value of non-overlapping patches of size (ds[0], ds[1]).\n",
    "        # Max pooling will be done over the 2 last dimensions.\n",
    "        \n",
    "        # I AM NOT VERY CLEAR ABOUT HOW PADDING WORKS AND HOW THIS 'ignore_border' parameter is to \n",
    "        # be used\n",
    "        pooledOut = pool.pool_2d(input = convOut, ds = self.poolSize, ignore_border=True, mode = 'max')\n",
    "        self.output = self.activationFn(pooledOut + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.outputDropout = self.output # no dropout in the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class softMaxLayer(object):\n",
    "\n",
    "    def __init__(self, nIn, nOut, pDropout=0.0):\n",
    "        self.nIn = nIn\n",
    "        self.nOut = nOut\n",
    "        self.pDropout = pDropout\n",
    "        # Initialize weights and biases\n",
    "        self.w = theano.shared(\n",
    "            np.zeros((nIn, nOut), dtype=theano.config.floatX),\n",
    "            name='w', borrow=True)\n",
    "        self.b = theano.shared(\n",
    "            np.zeros((nOut,), dtype=theano.config.floatX),\n",
    "            name='b', borrow=True)\n",
    "        self.params = [self.w, self.b]\n",
    "\n",
    "    def setInpt(self, inpt, inptDropout, miniBatchSize):\n",
    "        self.inpt = inpt.reshape((mini_batch_size, self.nIn))\n",
    "        self.output = softmax((1-self.pDropout)*T.dot(self.inpt, self.w) + self.b)\n",
    "        self.yOut = T.argmax(self.output, axis=1)\n",
    "        self.inptDropout = dropoutLayer(\n",
    "            inptDropout.reshape((miniBatchSize, self.nIn)), self.pDropout)\n",
    "        self.outputDropout = softmax(T.dot(self.inptDropout, self.w) + self.b)\n",
    "\n",
    "    def cost(self, net):\n",
    "        \"Return the log-likelihood cost.\"\n",
    "        return -T.mean(T.log(self.outputDropout)[T.arange(net.y.shape[0]), net.y])\n",
    "\n",
    "    def accuracy(self, y):\n",
    "        \"Return the accuracy for the mini-batch.\"\n",
    "        return T.mean(T.eq(y, self.yOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(layer, pDropout):\n",
    "    srng = shared_randomstreams.RandomStreams(\n",
    "        np.random.RandomState(0).randint(999999))\n",
    "    mask = srng.binomial(n=1, p=1-pDropout, size=layer.shape)\n",
    "    return layer*T.cast(mask, theano.config.floatX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
